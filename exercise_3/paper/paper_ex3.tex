\documentclass[a4paper,11pt]{article}
\usepackage[a4paper,left=3.1cm,right=3.1cm,top=3cm,bottom=2.9cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amssymb,amsfonts,stmaryrd, wasysym}
\usepackage{color}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{multirow} %multirow in table

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{titling}
\usepackage{caption}
\usepackage{hyperref}
\captionsetup{font=footnotesize}

\newcommand{\subtitle}[1]{%
  \posttitle{%
    \par\end{center}
    \begin{center}\large#1\end{center}
    \vskip0.5em}%
}

%%%%%%%%%%%%%%%%% Kopf- & Fußzeile %%%%%%%%%%%%%%%%%
\usepackage[footsepline]{scrlayer-scrpage}
\pagestyle{scrheadings}
\clearscrheadfoot
\title{Machine Learning - Exercise 3}
\subtitle{Model Stealing/Extraction}
\author{Christian Hatschka, Daniel Fangl, Esra Ceylan}
\date{February 2021}
\ihead{\thetitle}
\ifoot{\newline \theauthor}
\ofoot{\newline \pagemark}
\setlength{\headheight}{15pt}
\setlength{\footheight}{29pt}

\begin{document}
\maketitle

\section{Group members}
    Daniel Fangl (01526097) has a bachelor in Software Engineering and is currently in the master course Software Engineering and Internet Computing. Christian Hatschka (01525634) and Esra Ceylan (01526801) both have a bachelor in Mathematics and are currently enrolled in the master course Logic and Computation.
    
\section{Introduction}
    As the construction of a good model is an expensive and time consuming task, the thought of training a model based on an existing but not explicitly given one is interesting. We will do this using the original model as a black-box and taking its predictions into account.
    
    The aim of this project is to extract several models by using different model-stealing methods. Moreover, we will test their efficiency and accuracy and compare their performance to the performance of the underlying original model.
    
\section{Copycat Networks for CNN}\label{sec:copycat}
    This model stealing attack was proposed by Correia-Silva et al.~\cite{copycat} and aims to copy a Convolutional Neural Network. First, we will use the target CNN as a black-box to label our training set, which can also contain instances from the non-problem domain if the model allows us to classify them. For example the Microsoft Azure Emotion API does not allow to upload instances, where it cannot detect a face, hence which are not part of the problem-domain. Even images containing a face can sometimes be rejected as the Azure API could not always detect a face. By classifying the data using the target network as black-box we create a fake data set which will be used to train the extracted model.
    
    Next, we will select a model architecture of the copycat network. A possible problem with this approach is that we might not know the model architecture of the original network. However, this should not be a big problem as the knowledge can be copied to a different model.
    As pre-trained models tend to perform better, we will use the VGG-16 architecture with pre-trained weights. It is obviously optimal if the pre-trained model is already close to the problem domain.
    
    In the next step we will use the generated fake data set to fit the pre-trained model.
    As described in~\cite{copycat} we will use Stochastic Gradient Descent with a Step Down policy for the learning rate for training the copycat model. 
    
    We carried out our experiments using an Intel Core i7 6700k with 16GB RAM and NVIDIA GeForce GTX 1070. The environment of the experiments was Arch Linux with NVIDIA CUDA Framework ... and cuDNN .... \todo{!}
    %Paper: Intel Core i5-7500 3.40GHz with 32GiB of RAM and NVIDIA GeForce GTX 1070 with 8GiB of memory. The environment of the experiments was GNU/Linux Mint 18.1, with NVIDIA CUDA Framework 9.0 and cuDNN 7.0
    
    \subsection{Data Set: Emotion Detection}
        Unfortunately, we could not get access to the data sets used in~\cite{copycat} to compare our results with the results in the paper. Therefore, we used other data sets for our experiments. We combined two sets, which are described in the following two subsections to obtain a larger number of instances.
        %As our first data set we combined the data sets described in the following two subsections. 
        %Since KDEF and AKDEF were one of the used data sets in the paper of Correia-Silva et al. we wanted to take them as validation that we achieve similar results. Unfortunately, they contain only about 5\,000 samples in total and we could not get access to other data sets used in the paper. Therefore, we combined them with a larger data set.
        
        \subsubsection{KDEF and AKDEF}
            The Karolinska directed emotional faces data set~\cite{kdef} (short KDEF) contains 4\,900 images of human facial expressions. The pictures were taken of 70 different people (35 male and 35 female) displaying 7 different emotional expressions. All people portrayed were amateur actors between the ages of 20 and 30 and had no facial hair, glasses or makeup during the photos. Each of the expressions were taken from multiple different angles. 
            
            The averaged Karolinska directed emotional faces data set~\cite{kdef} (short AKDEF) contains $70$ averaged images of KDEF. The averaging was done over images containing the same gender, angle and emotional expression.
            
            Both data sets contain labels and were made by the Karolinska Institute in Stockholm, Sweden by the Department of Clinical Neuroscience in 1998. The data sets have been used in a large number of research publications on the topic. 
            
        \subsubsection{Emotion Detection from Facial Expressions}
            This data set was taken from the Kaggle competition 'Emotion Detection From Facial Expressions'~\cite{kaggle-emotion}. 
            It contains 13\,690 images of several human faces with different emotions. The data set was constructed from a variety of different sources, e.g.
            \begin{itemize}
                \item labeled Faces In The Wild,
                \item the Japanese Female Facial Expression (JAFFE) Database,
                \item Indian Movie Face database (IMFDB),
                \item the Extended Yale Face Database B.
            \end{itemize}
            
            Additionally, this data set contains, like the previous one, labeled data allowing us to compare the performance of the extracted model and also the overall performance of the target network.
    
    \subsection{Microsoft Azure Emotion API}
        As our first target model we used the publicly available Microsoft Azure Emotion API. For an input image this API returns a percentage vector of emotions for each face in the image. The following emotions are measured: 
        %This API classifies the emotion a face portrays, in the form of a percentage vectors that assigns values to the following emotions:
            \begin{itemize}
                \item Anger
                \item Contempt
                \item Disgust
                \item Fear
                \item Happiness
                \item Neutral
                \item Sadness
                \item Surprise
            \end{itemize}
        For example the face in Figure~\ref{fig:happy_face} from the publicly available KDEF data set returns a vector assigning a percentage of $0.999$ to happiness, $0.001$ to surprise and $0$ to everything else.
            \begin{figure}[h!]
                    \centering      \includegraphics[scale=0.2]{exercise_3/paper/images/AM18HAS.JPG}
                    \caption{Face classified as happy by the Azure Emotion API}
                    \label{fig:happy_face}
            \end{figure}
        
        In the Microsoft Azure Emotion API there is a free pricing option which allows us to submit up to 20 images per minute as well as a standard plan with up to 10 calls per second. As Microsoft has a promotion for students, which gives 100€ free credit, we chose the premium plan. %which allows us to submit up to 10 images per second.
        
        We decided to steal the model behind this API because this is also considered in the paper by Correia-Silva et al.~\cite{copycat}. Hence, we are able to assess whether the performance of our extracted classifier is similar to the performance of the extracted classifier in the paper.
        
       As mentioned in the beginning of Section~\ref{sec:copycat} Azure could not recognise all of the faces in our data set and hence could not classify for each face an emotion. Therefore, we added to the 8 emotions a further class called 'no\textunderscore face'. Figure~\ref{subfig:emotions_original-labels} shows the proportion of the different classes in our data set. In Figure~\ref{subfig:emotions_azure-labels} the distribution after classification by Azure. As one can see Azure could not detect about $13\%$ of the human faces.
       
       \begin{figure}[h!]
            \centering
            \begin{subfigure}[c]{0.45\textwidth}
                \centering
                \includegraphics[width=0.95\textwidth]{exercise_3/paper/images/emotions_original-labels.png}
                \caption{Class Distribution}
                \label{subfig:emotions_original-labels}
            \end{subfigure}
            \begin{subfigure}[c]{0.45\textwidth}
                \centering
                \includegraphics[width=0.95\textwidth]{exercise_3/paper/images/emotions_azure-labels.png}
                \caption{Classification of Azure}
                \label{subfig:emotions_azure-labels}
            \end{subfigure}
            \caption{Performance of Copycat Attack on first data set}
            \label{fig:emotions_distribution-labels}
        \end{figure}
        
        %Originally, we wanted to add instances, which are not part of the problem domain to our fake data set. However, as Azure does not classify images in which it does not detect any face, this was not possible.
        
    \subsection{Evaluation of the extracted Emotion Detection model}
        Since we had labeled data, we were able to compute the accuracy of the Microsoft Azure Emotion API. On the whole data set we obtained an accuracy of $0.76$.
        
        For testing the efficiency of our extracted model, we initially did a $75\%/25\%$ training/test split on our data set consisting of KDEF, AKDEF and the one from Kaggle. As we had labeled data, we had to delete the labels before querying the Azure API as a black-box. From now on we will refer to the split for training the extracted model as attack set, since this set will be used to attack the target network.
        
        To learn the effect of the number if images in the training set, we used an incremental approach. We trained the model on progressively larger subsets of the training set, which were included in each other. Starting with a random sample $s_0$ of $2\,000$ images, the next training set $s_n$ consisted of $s_{n-1}$ and additional $1\,000$ images.
        
        In order to compare the performance of our extracted models to the target network we considered two measures:
        \begin{itemize}
            \item Accuracy: correctly classified faces with respect to the original labels.
            \item Equality: correctly classified faces with respect to the target network.  
        \end{itemize}
        %The CNN-classifier was trained by a Multi-layer Perceptron. As the copying algorithm used for this classifier was rather basic we mapped the size of the training set to the size of the subset of AKDEF/KDEF used to train our copy-cat.
        
        %In order to compare the performance of our copied classifier to the original we split the data set into a test and training set and looked at the accuracy resp. equality of our copied model, dependant on the size of the set used to train the model. We trained the model on progressively larger subsets of the training set, which were included in each other. 
        
        Figure~\ref{fig:performance-azure} shows the results of our tests. Overall it can be seen that the accuracy as well as the equality improve for larger training sets. The best value for the accuracy in our tests was $0.578$. which seems much better compared to the results from the paper with about $0.35$. We also computed the accuracy of the Azure Emotion API on the test set and obtained an accuracy of $0.764$.
        
        However, the extracted model using the whole training set was was able to coincide on $70\%$ on the test data. 
        
        \begin{figure}[h!]
            \centering
            \begin{subfigure}[c]{0.49\textwidth}
                \centering
                \includegraphics[width=1\textwidth]{exercise_3/paper/images/accuracy_copy_Azure.png}
                \caption{Accuracy of the extracted models}
                \label{fig:Accuracy_Azure}
            \end{subfigure}
            \begin{subfigure}[c]{0.49\textwidth}
                \centering
                \includegraphics[width=1\textwidth]{exercise_3/paper/images/equality_copy_Azure.png}
                \caption{Equality of the extracted models}
                \label{fig:Equality_Azure}
            \end{subfigure}
            \caption{Performance of Copycat Attack on first data set}
            \label{fig:performance-azure}
        \end{figure}
        
        We took the offer for students and got $100$\$ of credit for free, which we used for the standard option. Therefore, we could label up to $10$ images per second. 
        
        Extracting a model using the whole data set took roughly one hour. About a third of this time was needed for training the model. The remaining time was for querying our target network for the labeling of our data set. 
        
        Using the free tier in Azure we would have needed about $16$ hours of time which is compared to the standard tier much longer, but still a reasonable timeframe. 

    \subsection{Data Set: Cats vs. Dogs}
        In the following we have a binary classification problem, where we want to predict if a certain image contains a cat or a dog. 
        
        For building a model we will again combine several data sets. First of all, we need a training set for training the target network. Additionally, we will need images for the attack resp. test set. 
        
        \subsubsection{Dogs vs. Cats}
            Instead of using an already existing API, in this experiment we chose to train our own CNN to steal to make matters more interesting. We obtained the training set for building our target model from the Kaggle competition 'Dogs vs. Cats'~\cite{dogs-cats}. It contains $25\,000$ images of cats and dogs including its labels.
            
            Additionally, this Kaggle competition provided another data set with $12\,500$ pictures of cats and dogs, but without labels, as the goal of the competition was to predict these labels. Since the attack set does not need any labeled data, we decided to use this set as part of the attack set.
            
        \subsubsection{Cats}
            This data set is also from Kaggle~\cite{cats}. It contains roughly $10\,000$ images solely of cats in various positions. 
            In an additional file more information, like pointers to the eyes, ears and nose of the cats in the pictures, can be found. However we did not use them in any capacity. 
            
            We used this data set as part of the attack and test set. 
            
        \subsubsection{Stanford dogs data set}
            The Stanford dogs data set ~\cite{dogs} was built using images from ImageNet in order to help categorize images. It contains $20\,580$ pictures purely of dogs. Additionally, it contains also classification for different breeds of dogs, but once again we did not use this information in out attack. 
            
            In our experiment this data set was used in the attack and test set.
            
        \subsubsection{Common objects in context data set}
            Since we construct the target network in this experiment by ourselves, we can use the copycat attack to it's full power and include non-problem domain data in our attack set. 
            For this we chose the COCO (Common Objects in Context) data set~\cite{coco}, which is known for object detection, segmentation and captioning. With a size of about $118\,000$ images it is a large data set. 
            
            These images will only be used in the attack set, since the classification problem is to decide if a picture shows a dog or a cat, but not to decide if it is something else. Moreover, since the images in this data set do not contain to our problem domain and compared to the number of our problem-domain data is much bigger, we decided not to use the whole data set. 
            
    \subsection{Evaluation of the extracted Cats vs. Dogs model}
        We tested the accuracy of the set by mixing the cat data set and the Stanford dog data set and picking $7\,644$ random images out of it. The resulting accuracy was $0,821$.
        
        -------------------------------------------------------------------
        
        We extracted the model in two different ways. In one instance we used only images that are part of the domain, therefore pictures of cats and dogs, in the attack set. In the other instance we used a mixture of non-domain images and domain images in a $2$ to $5$ ratio, to let the original model confess its information. 
    
        We start by looking at the accuracy of the extracted model in both cases:
            \begin{figure}[h!]
            \centering
            \begin{subfigure}[c]{0.49\textwidth}
                \centering
                \includegraphics[width=1\textwidth]{exercise_3/paper/images/Accuracy_copy_cat_domain.png}
                \caption{Accuracy using only problem domain}
                \label{fig:Accuracy_Cat_domain}
            \end{subfigure}
            \begin{subfigure}[c]{0.49\textwidth}
                \centering
                \includegraphics[width=1\textwidth]{exercise_3/paper/images/Accuracy_copy_cat.png}
                \caption{Accuracy using mixed instances }
                \label{fig:Accuracy_cat_mix}
            \end{subfigure}
            \caption{Accuracy of Copycat Attack on second data set}
            \label{fig:accuracy_cat}
        \end{figure}
        It is visible, that in case of the attack set being of similar size the attack set consisting only of instances, that are part of the problem domain performed better. 

        Looking at how much of the data our extracted model labeled the same as the original model, showed the same:
         \begin{figure}[h!]
            \centering
            \begin{subfigure}[c]{0.49\textwidth}
                \centering
                \includegraphics[width=1\textwidth]{exercise_3/paper/images/Equality_copy_cat_domain.png}
                \caption{Equality using only problem domain}
                \label{fig:Equality_cat_domain}
            \end{subfigure}
            \begin{subfigure}[c]{0.49\textwidth}
                \centering
                \includegraphics[width=1\textwidth]{exercise_3/paper/images/Equality_copy_cat.png}
                \caption{Equality using mixed instances}
                \label{fig:Equality_cat_mix}
            \end{subfigure}
            \caption{Equality of Copycat Attack on second data set}
            \label{fig:equality_cat}
        \end{figure}
    Once again using only problem domain data performed better, when looking at attack sets of similar size.
    
    The time needed to train the models grew roughly linearly with the size of the attack sets. The largest attack set of size $94\,572$ trained the extracted model in roughly $24$ minutes. Below is the time needed to construct the extracted model compared to the size of the attack set.
        \begin{figure}[h!]
            \centering      \includegraphics[scale=0.2]{exercise_3/paper/images/Time_copy_cat.png}
            \caption{Time needed to extract a model}
            \label{fig:time_cat}
        \end{figure}
            
\section{Decision Tree Path-Finding Attack}
    As the name suggests, this model extraction method targets decision tree models. Such a model divides the input space into several discrete partitions. The idea behind the decision tree path-finding attack is to vary the values of each feature bit by bit to find the value at which the tree splits.
    
    
    
    We carried out our experiments using GCP e2-highmem-16 offered by the Google Cloud Platform running on Ubuntu 20.04.
    
    \subsection{Data Set}
        ...
        
    \subsection{Evaluation}
        ...

\section{Conclusions}
    The copy-cat method in general performed extremely well, getting an equality of roughly $0,7$ even on the rather complex Microsoft Azure emotional API in less than one hour and using a rather small attack set. While using only instances that are part of the problem domain worked better on similar attack set sizes, finding instances that are not part of the problem domain is an easier task in general and still works pretty well. Taking this into consideration, in case that the number of queries on can make to the blackbox is severely limited or run time is an important factor, one should rely mainly on problem domain instances. Should this not be the case non-problem domain instances allow for a quick assembly of a large data set that can extract a model reasonably well.


\bibliographystyle{abbrv}
\bibliography{literature}

\end{document}